{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Installing and Using NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download() # download all packages/collections in nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Sampath, how are you doing today?', 'The weather is great and NLTK is interesting.', 'It is snowing here and really, it is pretty cold.', 'Though at -5 degrees, actually quite warmer than usual!', 'Okay, bye now.', 'ttyl.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "example_text = \"Hello Mr. Sampath, how are you doing today? The weather is great and NLTK is interesting. It is snowing here and really, it is pretty cold. Though at -5 degrees, actually quite warmer than usual! Okay, bye now. ttyl.\"\n",
    "print(sent_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr.', 'Sampath', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', 'and', 'NLTK', 'is', 'interesting', '.', 'It', 'is', 'snowing', 'here', 'and', 'really', ',', 'it', 'is', 'pretty', 'cold', '.', 'Though', 'at', '-5', 'degrees', ',', 'actually', 'quite', 'warmer', 'than', 'usual', '!', 'Okay', ',', 'bye', 'now', '.', 'ttyl', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Stop Words\n",
    "Stop words are words which are filtered out before or after processing of natural language data. Stop words usually refers to the most common words in a language.\n",
    "\n",
    "(Note: there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'example', 'sentence', 'showing', 'stop', 'word', 'filtration', '.', 'Hopefully', ',', 'work', '!', 'Or', ',', '?', 'Wont', '?', '?', 'Lets', 'find', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "example_sentence = \"This is an example sentence showing stop word filtration. Hopefully, this will work! Or, will it? Wont it?? Lets find out!!\"\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "words = word_tokenize(example_sentence)\n",
    "filtered_sentence = []\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "# The above for loop as a one-liner\n",
    "# filtered_sentence = [w for w in words if w not in stop_words]\n",
    "\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Reducing different forms of a word to a common base form. For example: \n",
    "- “I am a student” = “I be a student”;  \n",
    "- “My dog’s fur is dark” = “My dog fur be dark”.\n",
    "\n",
    "Note: Stemming may not be needed aynmore with modern NLP tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n",
      "I\n",
      "t\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "v\n",
      "e\n",
      "r\n",
      "y\n",
      " \n",
      "i\n",
      "m\n",
      "p\n",
      "o\n",
      "r\n",
      "t\n",
      "a\n",
      "n\n",
      "t\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "b\n",
      "e\n",
      " \n",
      "p\n",
      "y\n",
      "t\n",
      "h\n",
      "o\n",
      "n\n",
      "l\n",
      "y\n",
      " \n",
      "w\n",
      "h\n",
      "i\n",
      "l\n",
      "e\n",
      " \n",
      "y\n",
      "o\n",
      "u\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      " \n",
      "p\n",
      "y\n",
      "t\n",
      "h\n",
      "o\n",
      "n\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "w\n",
      "i\n",
      "t\n",
      "h\n",
      " \n",
      "p\n",
      "y\n",
      "t\n",
      "h\n",
      "o\n",
      "n\n",
      ".\n",
      " \n",
      "A\n",
      "l\n",
      "l\n",
      " \n",
      "p\n",
      "y\n",
      "t\n",
      "h\n",
      "o\n",
      "n\n",
      "e\n",
      "r\n",
      "s\n",
      " \n",
      "h\n",
      "a\n",
      "v\n",
      "e\n",
      " \n",
      "p\n",
      "y\n",
      "t\n",
      "h\n",
      "o\n",
      "n\n",
      "e\n",
      "d\n",
      " \n",
      "b\n",
      "a\n",
      "d\n",
      "l\n",
      "y\n",
      " \n",
      "a\n",
      "t\n",
      " \n",
      "l\n",
      "e\n",
      "a\n",
      "s\n",
      "t\n",
      " \n",
      "o\n",
      "n\n",
      "c\n",
      "e\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer # Using the Porter Stemming Algorithm from 1979!\n",
    "from nltk.tokenize import word_tokenize\n",
    "ps = PorterStemmer()\n",
    "\n",
    "example_words = [\"python\",\"pythoning\",\"pythoner\",\"pythoned\",\"pythonly\"]\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))\n",
    "\n",
    "example_sentence = \"It is very important to be pythonly while you are pythoning with python. All pythoners have pythoned badly at least once.\"\n",
    "words = word_tokenize(example_sentence)\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
